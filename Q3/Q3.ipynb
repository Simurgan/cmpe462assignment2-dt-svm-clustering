{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to only include samples with labels 2, 3, 8, and 9\n",
    "filtered_train_indices = torch.tensor([idx for idx, label in enumerate(trainset.targets) if label in [2, 3, 8, 9]])\n",
    "filtered_trainset = torch.utils.data.Subset(trainset, filtered_train_indices)\n",
    "flatten_trainset = torch.stack([image.view(-1) for image, label in filtered_trainset])\n",
    "train_targets = torch.tensor([label for _, label in filtered_trainset])  # Extract target values\n",
    "\n",
    "filtered_test_indices = torch.tensor([idx for idx, label in enumerate(testset.targets) if label in [2, 3, 8, 9]])\n",
    "filtered_testset = torch.utils.data.Subset(testset, filtered_test_indices)\n",
    "flatten_testset = torch.stack([image.view(-1) for image, label in filtered_testset])\n",
    "test_targets = torch.tensor([label for _, label in filtered_testset])  # Extract target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "trainset_normalized = normalize(flatten_trainset)\n",
    "testset_normalized = normalize(flatten_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans algorithm using euclidean distance\n",
    "def kmeans_euclidean(data, max_iters=1000, k=[2, 3, 8, 9]):\n",
    "    # Initialize centroids randomly\n",
    "    centroids = {label: data[np.random.choice(range(data.shape[0]))] for label in k}\n",
    "    \n",
    "    # Create a mapping for labels\n",
    "    label_mapping = {i: label for i, label in enumerate(k)}\n",
    "    \n",
    "    while(1):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        distances = np.sqrt(((data - np.array(list(centroids.values()))[:, np.newaxis])**2).sum(axis=2))\n",
    "        labels = np.argmin(distances, axis=0)\n",
    "        \n",
    "        # Map the labels\n",
    "        labels = np.vectorize(label_mapping.get)(labels)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.array([data[labels == label].mean(axis=0) for label in k])\n",
    "    \n",
    "        # Check for convergence\n",
    "        if np.allclose(np.array(list(centroids.values())), new_centroids):\n",
    "            break\n",
    "            \n",
    "        centroids = {label: new_centroids[i] for i, label in enumerate(k)}\n",
    "    \n",
    "    return labels, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans algorithm using cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def kmeans_cosine(data, max_iters=100, k=[2, 3, 8, 9]):\n",
    "    # Initialize centroids randomly\n",
    "    centroids = {label: data[np.random.choice(range(data.shape[0]))] for label in k}\n",
    "    \n",
    "    # Create a mapping for labels\n",
    "    label_mapping = {i: label for i, label in enumerate(k)}\n",
    "    \n",
    "    while(1):\n",
    "        # Compute cosine similarity\n",
    "        dot_product = np.dot(data, centroids)\n",
    "        magnitude_a = np.linalg.norm(data)\n",
    "        magnitude_b = np.linalg.norm(centroids)\n",
    "        similarity = dot_product / (magnitude_a * magnitude_b)\n",
    "    \n",
    "        labels = np.argmax(similarity, axis=1)\n",
    "        \n",
    "        # Map the labels\n",
    "        labels = np.vectorize(label_mapping.get)(labels)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.array([data[labels == label].mean(axis=0) for label in k])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(np.array(list(centroids.values())), new_centroids):\n",
    "            break\n",
    "        \n",
    "        centroids = {label: new_centroids[i] for i, label in enumerate(k)}\n",
    "    \n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for SSE calculation \n",
    "def calculate_sse(data, centroids, assigned_labels):\n",
    "\n",
    "    sse = 0\n",
    "    for i in range (len(data)):\n",
    "        label = assigned_labels[i]\n",
    "        centroid = centroids[label]\n",
    "\n",
    "        distance = np.sqrt(((data[i] - centroid) ** 2).sum())\n",
    "        sse += distance\n",
    "\n",
    "    return sse\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for cluster accuracy calculation\n",
    "def compute_clustering_accuracy(predicted_labels, true_labels):\n",
    "\n",
    "    # Create a mapping between predicted labels and true labels\n",
    "    label_mapping = {}\n",
    "    for pred_label, true_label in zip(predicted_labels, true_labels):\n",
    "        if pred_label not in label_mapping:\n",
    "            label_mapping[pred_label] = true_label\n",
    "\n",
    "    # Assign true labels to predicted cluster labels\n",
    "    mapped_labels = [label_mapping[label] for label in predicted_labels]\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct_count = sum(1 for pred, true in zip(mapped_labels, true_labels) if pred == true)\n",
    "    total_count = len(true_labels)\n",
    "    accuracy = correct_count / total_count\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with Euclidean distance\n",
    "labels_euclidean, centroids_euclidean = kmeans_euclidean(trainset_normalized.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform k-means clustering using cosine similarity\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m labels_cosine, centroids_cosine \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_cosine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset_normalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate clustering accuracy using cosine similarity\u001b[39;00m\n\u001b[1;32m      5\u001b[0m accuracy_cosine \u001b[38;5;241m=\u001b[39m accuracy_score(train_targets, labels_cosine)\n",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m, in \u001b[0;36mkmeans_cosine\u001b[0;34m(data, max_iters, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m label_mapping \u001b[38;5;241m=\u001b[39m {i: label \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(k)}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Compute cosine similarity\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     dot_product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     magnitude_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(data)\n\u001b[1;32m     14\u001b[0m     magnitude_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(centroids)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'dict'"
     ]
    }
   ],
   "source": [
    "# Perform k-means clustering using cosine similarity\n",
    "labels_cosine, centroids_cosine = kmeans_cosine(trainset_normalized)\n",
    "\n",
    "# Calculate clustering accuracy using cosine similarity\n",
    "accuracy_cosine = accuracy_score(train_targets, labels_cosine)\n",
    "print(\"Clustering accuracy using cosine similarity:\", accuracy_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Error (SSE): 613.3626684006304\n"
     ]
    }
   ],
   "source": [
    "# Calculate SSE\n",
    "sse = calculate_sse(trainset_normalized.numpy(), centroids_euclidean, labels_euclidean)\n",
    "print(\"Sum of Squared Error (SSE):\", sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy (Euclidean): 0.6108669262003432\n"
     ]
    }
   ],
   "source": [
    "# Calculate clustering accuracy\n",
    "accuracy_euclidean = compute_clustering_accuracy(train_targets.numpy(), labels_euclidean)\n",
    "print(\"Clustering Accuracy (Euclidean):\", accuracy_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering accuracy: 0.5029092887940056\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(train_targets.numpy(), labels_euclidean)\n",
    "print(\"Clustering accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering accuracy using PCA-transformed data: 0.19276654527188244\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to extract features\n",
    "pca = PCA(n_components=50)  # You can adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(trainset_normalized)\n",
    "\n",
    "# Perform k-means clustering on the PCA-transformed data\n",
    "labels_pca, centroids_pca =  kmeans_euclidean(X_pca)\n",
    "\n",
    "# Calculate clustering accuracy using PCA-transformed data\n",
    "accuracy_pca = accuracy_score(train_targets, labels_pca)\n",
    "print(\"Clustering accuracy using PCA-transformed data:\", accuracy_pca)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
